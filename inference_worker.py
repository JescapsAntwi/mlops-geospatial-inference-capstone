"""
Inference Worker Module
This module simulates running the Palm model on GeoTIFF files.
In a real implementation, this would call the actual Palm model Docker container.
"""

import asyncio
import json
import random
from datetime import datetime
from typing import Dict, List, Any
import rasterio
from PIL import Image
import numpy as np
import onnxruntime as ort

class InferenceWorker:
    """
    Worker class for running inference on GeoTIFF files
    
    This is a simplified implementation that simulates the Palm model.
    In production, this would:
    1. Call the actual Palm model Docker container
    2. Handle proper image preprocessing
    3. Manage GPU resources
    4. Handle model outputs correctly
    """
    
    def __init__(self):
        # Load the ONNX model
        self.model_path = "models/model.onnx"
        self.session = ort.InferenceSession(self.model_path)
        self.input_name = self.session.get_inputs()[0].name
        self.output_name = self.session.get_outputs()[0].name
        # Simulated class labels for the Palm model
        self.class_labels = [
            "palm_tree", "building", "road", "water", "vegetation", 
            "vehicle", "person", "agriculture", "forest", "urban"
        ]
    
    def _run_onnx_inference(self, input_data: np.ndarray) -> List[Dict[str, Any]]:
        # Run inference using ONNX model
        outputs = self.session.run([self.output_name], {self.input_name: input_data})
        detections = []

        for i, output in enumerate(outputs[0]):
            # Assuming output contains bounding boxes, class IDs, and confidence scores
            bbox = output[:4]  # x_min, y_min, x_max, y_max
            class_id = int(output[4])
            confidence = float(output[5])

            detection = {
                "id": i,
                "class_id": class_id,
                "class_name": self.class_labels[class_id],
                "bbox": bbox.tolist(),
                "bbox_format": "xyxy",
                "confidence": confidence,
                "area": (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])
            }
            detections.append(detection)

        return detections
    
    async def process_file(self, file_path: str) -> Dict[str, Any]:
        """
        Process a single GeoTIFF file through the Palm model
        
        Args:
            file_path: Path to the GeoTIFF file
            
        Returns:
            Dictionary containing inference results
        """
        try:
            print(f"üîç Processing {file_path}...")
            
            # Read GeoTIFF data
            with rasterio.open(file_path) as src:
                image_data = src.read(1)  # Read the first band
                image_data = np.expand_dims(image_data, axis=0)  # Add batch dimension
                # Get image dimensions
                height, width = src.shape
                # Get coordinate system info
                crs = src.crs
                transform = src.transform
            
            # Run inference
            detections = self._run_onnx_inference(image_data)
            
            # Create result structure
            result = {
                "file_path": file_path,
                "file_name": file_path.split("/")[-1],
                "image_info": {
                    "width": width,
                    "height": height,
                    "crs": str(crs),
                    "transform": list(transform)[:6] if transform else None
                },
                "detections": detections,
                "processing_time": datetime.now().isoformat(),
                "model_version": "palm_v1.0"
            }
            
            print(f"‚úÖ Completed inference on {file_path}")
            return result
            
        except Exception as e:
            print(f"‚ùå Error processing {file_path}: {e}")
            # Return error result
            return {
                "file_path": file_path,
                "file_name": file_path.split("/")[-1],
                "error": str(e),
                "processing_time": datetime.now().isoformat()
            }
    
    def _simulate_palm_detections(self, width: int, height: int) -> List[Dict[str, Any]]:
        """
        Simulate detection results from the Palm model
        
        This generates realistic-looking bounding boxes and detections.
        In production, this would be the actual output from the Palm model.
        """
        detections = []
        
        # Generate random number of detections (1-10 per image)
        num_detections = random.randint(1, 10)
        
        for i in range(num_detections):
            # Random class selection
            class_id = random.randint(0, len(self.class_labels) - 1)
            class_name = self.class_labels[class_id]
            
            # Generate random bounding box coordinates
            # Ensure boxes are within image boundaries
            box_width = random.randint(20, min(200, width // 4))
            box_height = random.randint(20, min(200, height // 4))
            
            x_min = random.randint(0, width - box_width)
            y_min = random.randint(0, height - box_height)
            x_max = x_min + box_width
            y_max = y_min + box_height
            
            # Generate confidence score (0.5 to 0.99)
            confidence = round(random.uniform(0.5, 0.99), 3)
            
            detection = {
                "id": i,
                "class_id": class_id,
                "class_name": class_name,
                "bbox": [x_min, y_min, x_max, y_max],
                "bbox_format": "xyxy",  # x_min, y_min, x_max, y_max
                "confidence": confidence,
                "area": box_width * box_height
            }
            
            detections.append(detection)
        
        return detections
    
    async def process_batch(self, file_paths: List[str]) -> List[Dict[str, Any]]:
        """
        Process multiple files in parallel
        
        Args:
            file_paths: List of file paths to process
            
        Returns:
            List of inference results
        """
        # Process files concurrently for better performance
        tasks = [self.process_file(file_path) for file_path in file_paths]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Filter out exceptions and return valid results
        valid_results = []
        for result in results:
            if isinstance(result, Exception):
                print(f"‚ùå Batch processing error: {result}")
            else:
                valid_results.append(result)
        
        return valid_results
